{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9091e134",
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from catboost import CatBoostClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "273bf7da",
   "metadata": {},
   "outputs": [],
   "source": [
    "d = np.load(\"../data/training_data/splits.npz\", allow_pickle=True)\n",
    "X_train = d[\"X_train\"]\n",
    "X_test = d[\"X_test\"]\n",
    "y_train = d[\"y_train\"]\n",
    "y_test = d[\"y_test\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d5de1115",
   "metadata": {},
   "outputs": [],
   "source": [
    "d = np.load(\"../data/training_data/features_labels.npz\", allow_pickle=True)\n",
    "X = d[\"X\"]\n",
    "y = d[\"y\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fd209cf9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGBoost loaded\n",
      "LightGBM loaded\n",
      "CatBoost loaded\n"
     ]
    }
   ],
   "source": [
    "# Load XGBoost (assuming it was saved with joblib)\n",
    "xgboost_model = joblib.load('trained_models/xgb_model.pkl')\n",
    "print(\"XGBoost loaded\")\n",
    "\n",
    "# Load LightGBM\n",
    "lgbm_model = joblib.load('trained_models/lgbm_model.pkl')\n",
    "print(\"LightGBM loaded\")\n",
    "\n",
    "# Load CatBoost\n",
    "catboost_model = CatBoostClassifier().load_model('trained_models/catboost_model.cbm')\n",
    "print(\"CatBoost loaded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "582564e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hard Voting (Majority Rule):\n",
      "Hard Voting Accuracy: 0.9788\n",
      "Classification report: \n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00       425\n",
      "           1       0.99      0.96      0.98       379\n",
      "           2       0.96      0.97      0.97       397\n",
      "           3       0.96      0.98      0.97       399\n",
      "\n",
      "    accuracy                           0.98      1600\n",
      "   macro avg       0.98      0.98      0.98      1600\n",
      "weighted avg       0.98      0.98      0.98      1600\n",
      "\n",
      "\n",
      "Soft Voting (Average Probabilities):\n",
      "Soft Voting Accuracy: 0.9800\n",
      "Classification report: \n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00       425\n",
      "           1       0.99      0.97      0.98       379\n",
      "           2       0.97      0.97      0.97       397\n",
      "           3       0.96      0.98      0.97       399\n",
      "\n",
      "    accuracy                           0.98      1600\n",
      "   macro avg       0.98      0.98      0.98      1600\n",
      "weighted avg       0.98      0.98      0.98      1600\n",
      "\n",
      "\n",
      "Comparison:\n",
      "XGBoost     : 0.9750\n",
      "LightGBM    : 0.9844\n",
      "CatBoost    : 0.9756\n",
      "Hard Voting  : 0.9788\n",
      "Soft Voting  : 0.9800\n"
     ]
    }
   ],
   "source": [
    "# 1. SIMPLE HARD VOTING (Majority Vote)\n",
    "print(\"Hard Voting (Majority Rule):\")\n",
    "\n",
    "# Get predictions from each model\n",
    "pred_xgb = xgboost_model.predict(X_test)\n",
    "pred_lgb = lgbm_model.predict(X_test)\n",
    "pred_cat = catboost_model.predict(X_test)\n",
    "\n",
    "if len(pred_xgb.shape) > 1:\n",
    "    pred_xgb = pred_xgb.flatten()\n",
    "if len(pred_lgb.shape) > 1:\n",
    "    pred_lgb = pred_lgb.flatten()\n",
    "if len(pred_cat.shape) > 1:\n",
    "    pred_cat = pred_cat.flatten()\n",
    "\n",
    "# Take majority vote\n",
    "hard_vote_pred = []\n",
    "for i in range(len(X_test)):\n",
    "    votes = [pred_xgb[i], pred_lgb[i], pred_cat[i]]\n",
    "    # Most common vote wins\n",
    "    hard_vote_pred.append(np.bincount(votes).argmax())\n",
    "\n",
    "# Evaluate\n",
    "hard_acc = accuracy_score(y_test, hard_vote_pred)\n",
    "print(f\"Hard Voting Accuracy: {hard_acc:.4f}\")\n",
    "print(f\"Classification report: \\n\")\n",
    "print(classification_report(y_test, hard_vote_pred))\n",
    "\n",
    "# 2. SIMPLE SOFT VOTING (Average Probabilities)\n",
    "print(\"\\nSoft Voting (Average Probabilities):\")\n",
    "\n",
    "# Get probability predictions\n",
    "prob_xgb = xgboost_model.predict_proba(X_test)\n",
    "prob_lgb = lgbm_model.predict_proba(X_test) \n",
    "prob_cat = catboost_model.predict_proba(X_test)\n",
    "\n",
    "# Average the probabilities\n",
    "avg_probs = (prob_xgb + prob_lgb + prob_cat) / 3\n",
    "\n",
    "# Take class with highest average probability\n",
    "soft_vote_pred = np.argmax(avg_probs, axis=1)\n",
    "\n",
    "# Evaluate\n",
    "soft_acc = accuracy_score(y_test, soft_vote_pred)\n",
    "print(f\"Soft Voting Accuracy: {soft_acc:.4f}\")\n",
    "print(f\"Classification report: \\n\")\n",
    "print(classification_report(y_test, soft_vote_pred))\n",
    "\n",
    "# Compare with individual models\n",
    "print(\"\\nComparison:\")\n",
    "for name, model in [('XGBoost', xgboost_model), ('LightGBM', lgbm_model), ('CatBoost', catboost_model)]:\n",
    "    acc = accuracy_score(y_test, model.predict(X_test))\n",
    "    print(f\"{name:12}: {acc:.4f}\")\n",
    "print(f\"Hard Voting  : {hard_acc:.4f}\")\n",
    "print(f\"Soft Voting  : {soft_acc:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "b2a00a41",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. Getting probability predictions from training data...\n",
      "2. Stacking predictions to create new training features...\n",
      "   Original training shape: (6400, 16)\n",
      "   Stacked training shape: (6400, 12)\n",
      "3. Doing the same for test data...\n",
      "   Stacked test shape: (1600, 12)\n",
      "4. Training meta-learner on stacked features...\n",
      "   Meta-learner trained!\n",
      "5. Making final predictions...\n",
      "\n",
      "Stacking Ensemble Accuracy: 0.9800\n",
      "Classification report: \n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00       425\n",
      "           1       0.99      0.97      0.98       379\n",
      "           2       0.97      0.97      0.97       397\n",
      "           3       0.96      0.98      0.97       399\n",
      "\n",
      "    accuracy                           0.98      1600\n",
      "   macro avg       0.98      0.98      0.98      1600\n",
      "weighted avg       0.98      0.98      0.98      1600\n",
      "\n",
      "\n",
      "Comparison\n",
      "XGBoost   : 0.9750\n",
      "LightGBM  : 0.9844\n",
      "CatBoost  : 0.9756\n",
      "Stacking Ensemble: 0.9800\n",
      "\n",
      "Best accuracy: LightGBM (0.9844)\n"
     ]
    }
   ],
   "source": [
    "# 1. Get probability predictions from each model on TRAINING data\n",
    "print(\"1. Getting probability predictions from training data...\")\n",
    "\n",
    "prob_xgb_train = xgboost_model.predict_proba(X_train)\n",
    "prob_lgb_train = lgbm_model.predict_proba(X_train) \n",
    "prob_cat_train = catboost_model.predict_proba(X_train)\n",
    "\n",
    "# 2. Stack them horizontally to create new training features\n",
    "print(\"2. Stacking predictions to create new training features...\")\n",
    "\n",
    "# Each model gives [n_samples, n_classes] predictions\n",
    "# Stack them: [model1_class1, model1_class2, ..., model2_class1, model2_class2, ...]\n",
    "stacked_train = np.hstack([prob_xgb_train, prob_lgb_train, prob_cat_train])\n",
    "\n",
    "print(f\"   Original training shape: {X_train.shape}\")\n",
    "print(f\"   Stacked training shape: {stacked_train.shape}\")\n",
    "\n",
    "# 3. Do the same for TEST data\n",
    "print(\"3. Doing the same for test data...\")\n",
    "\n",
    "prob_xgb_test = xgboost_model.predict_proba(X_test)\n",
    "prob_lgb_test = lgbm_model.predict_proba(X_test)\n",
    "prob_cat_test = catboost_model.predict_proba(X_test)\n",
    "\n",
    "stacked_test = np.hstack([prob_xgb_test, prob_lgb_test, prob_cat_test])\n",
    "print(f\"   Stacked test shape: {stacked_test.shape}\")\n",
    "\n",
    "# 4. Train a simple model on these stacked features\n",
    "print(\"4. Training meta-learner on stacked features...\")\n",
    "\n",
    "# Use Logistic Regression as the meta-learner\n",
    "meta_model = LogisticRegression(\n",
    "    max_iter=1000,  # Increase iterations for convergence\n",
    "    random_state=42,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "meta_model.fit(stacked_train, y_train)\n",
    "print(\"   Meta-learner trained!\")\n",
    "\n",
    "# 5. Make final predictions\n",
    "print(\"5. Making final predictions...\")\n",
    "\n",
    "stacking_predictions = meta_model.predict(stacked_test)\n",
    "stacking_accuracy = accuracy_score(y_test, stacking_predictions)\n",
    "\n",
    "print(f\"\\nStacking Ensemble Accuracy: {stacking_accuracy:.4f}\")\n",
    "print(f\"Classification report: \\n\")\n",
    "print(classification_report(y_test, stacking_predictions))\n",
    "\n",
    "# 6. Compare with individual models\n",
    "print(\"\\nComparison\")\n",
    "\n",
    "# Get individual model accuracies\n",
    "indiv_results = []\n",
    "\n",
    "# XGBoost\n",
    "xgb_pred = xgboost_model.predict(X_test)\n",
    "xgb_acc = accuracy_score(y_test, xgb_pred)\n",
    "indiv_results.append(('XGBoost', xgb_acc))\n",
    "\n",
    "# LightGBM\n",
    "lgb_pred = lgbm_model.predict(X_test)\n",
    "lgb_acc = accuracy_score(y_test, lgb_pred)\n",
    "indiv_results.append(('LightGBM', lgb_acc))\n",
    "\n",
    "# CatBoost\n",
    "cat_pred = catboost_model.predict(X_test)\n",
    "cat_acc = accuracy_score(y_test, cat_pred)\n",
    "indiv_results.append(('CatBoost', cat_acc))\n",
    "\n",
    "# Display results\n",
    "for name, acc in indiv_results:\n",
    "    print(f\"{name:10}: {acc:.4f}\")\n",
    "\n",
    "print(f\"Stacking Ensemble: {stacking_accuracy:.4f}\")\n",
    "\n",
    "# Find best\n",
    "all_results = dict(indiv_results)\n",
    "all_results['Stacking'] = stacking_accuracy\n",
    "\n",
    "best_name = max(all_results, key=all_results.get)\n",
    "best_acc = all_results[best_name]\n",
    "\n",
    "print(f\"\\nBest accuracy: {best_name} ({best_acc:.4f})\")\n",
    "\n",
    "if best_name == 'Stacking':\n",
    "    improvement = best_acc - max(xgb_acc, lgb_acc, cat_acc)\n",
    "    print(f\"Improvement over best individual model: {improvement:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "05cfb39d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hard_voting_report.csv saved\n",
      "soft_voting_report.csv saved\n",
      "stacking_report.csv saved\n"
     ]
    }
   ],
   "source": [
    "# Save all reports\n",
    "for name, pred in [('hard_voting', hard_vote_pred), \n",
    "                   ('soft_voting', soft_vote_pred), \n",
    "                   ('stacking', stacking_predictions)]:\n",
    "    report = classification_report(y_test, pred, output_dict=True)\n",
    "    df = pd.DataFrame(report).transpose()\n",
    "    df.to_csv(f\"performance_metrics/ensemble_classification_reports/{name}_report.csv\")\n",
    "    print(f\"{name}_report.csv saved\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a8c7f10",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "chan_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
